{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('perf.csv')\n",
        "y_true = df['yval']\n",
        "y_prob = df['ypred']\n"
      ],
      "metadata": {
        "id": "DSXr5owJnaCG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to compute metrics at any threshold\n",
        "def compute_metrics(y_true, y_prob, threshold):\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "    SEN = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity\n",
        "    SPE = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
        "    PPV = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value\n",
        "    NPV = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
        "    F1 = f1_score(y_true, y_pred)\n",
        "    return SEN, SPE, PPV, NPV, F1\n"
      ],
      "metadata": {
        "id": "bfqeh3vSnbR4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the threshold where Sensitivity is closest to 0.98\n",
        "# Search for the threshold where sensitivity is closest to 0.98\n",
        "best_diff = 1\n",
        "best_threshold = None\n",
        "best_metrics = None\n",
        "\n",
        "# Use 2000 evenly spaced thresholds between 0 and 1\n",
        "for thresh in np.linspace(0, 1, 2000):\n",
        "    SEN, SPE, PPV, NPV, F1 = compute_metrics(y_true, y_prob, thresh)\n",
        "    diff = abs(SEN - 0.98)\n",
        "    if diff < best_diff:\n",
        "        best_diff = diff\n",
        "        best_threshold = thresh\n",
        "        best_metrics = (SEN, SPE, PPV, NPV, F1)\n",
        "\n",
        "# Round results to 4 decimal digits\n",
        "threshold_1a = round(best_threshold, 4)\n",
        "SEN, SPE, PPV, NPV, F1 = [round(x, 4) for x in best_metrics]\n",
        "\n",
        "print(f\"Threshold: {threshold_1a}\")\n",
        "print(f\"Sensitivity (SEN): {SEN}\")\n",
        "print(f\"Specificity (SPE): {SPE}\")\n",
        "print(f\"PPV: {PPV}\")\n",
        "print(f\"NPV: {NPV}\")\n",
        "print(f\"F1 score: {F1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVulPKGSne8W",
        "outputId": "4c9a20e8-0b97-4ca9-8492-fc28ff328f51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.1051\n",
            "Sensitivity (SEN): 0.9816\n",
            "Specificity (SPE): 0.0739\n",
            "PPV: 0.3875\n",
            "NPV: 0.8706\n",
            "F1 score: 0.5556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('perf.csv')\n",
        "y_true = df['yval']\n",
        "y_prob = df['ypred']\n",
        "\n",
        "# Function to compute metrics at any threshold\n",
        "def compute_metrics(y_true, y_prob, threshold):\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    SEN = tp / (tp + fn) if (tp + fn) > 0 else 0    # Sensitivity\n",
        "    SPE = tn / (tn + fp) if (tn + fp) > 0 else 0    # Specificity\n",
        "    PPV = tp / (tp + fp) if (tp + fp) > 0 else 0    # Positive Predictive Value\n",
        "    NPV = tn / (tn + fn) if (tn + fn) > 0 else 0    # Negative Predictive Value\n",
        "    F1 = f1_score(y_true, y_pred)\n",
        "    return SEN, SPE, PPV, NPV, F1\n",
        "\n",
        "# Loop through 2000 thresholds to find the one where specificity is closest to 0.98\n",
        "best_diff = 1\n",
        "best_threshold = None\n",
        "best_metrics = None\n",
        "\n",
        "for thresh in np.linspace(0, 1, 2000):  # 2000 evenly spaced thresholds from 0 to 1\n",
        "    SEN, SPE, PPV, NPV, F1 = compute_metrics(y_true, y_prob, thresh)\n",
        "    diff = abs(SPE - 0.98)  # Now compare specificity\n",
        "    if diff < best_diff:\n",
        "        best_diff = diff\n",
        "        best_threshold = thresh\n",
        "        best_metrics = (SEN, SPE, PPV, NPV, F1)\n",
        "\n",
        "# Round results to 2 decimal digits for reporting\n",
        "threshold_1b = round(best_threshold, 2)\n",
        "SEN, SPE, PPV, NPV, F1 = [round(x, 2) for x in best_metrics]\n",
        "\n",
        "print(f\"Threshold: {threshold_1b}\")\n",
        "print(f\"Sensitivity (SEN): {SEN}\")\n",
        "print(f\"Specificity (SPE): {SPE}\")\n",
        "print(f\"PPV: {PPV}\")\n",
        "print(f\"NPV: {NPV}\")\n",
        "print(f\"F1 score: {F1}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9e4JWMeduQr",
        "outputId": "4aac4904-a3f2-4a72-f278-3c0a835e6352"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.81\n",
            "Sensitivity (SEN): 0.18\n",
            "Specificity (SPE): 0.98\n",
            "PPV: 0.84\n",
            "NPV: 0.67\n",
            "F1 score: 0.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.c\n",
        "row_c = results_df.sort_values('F1', ascending=False).iloc[0]\n",
        "\n",
        "final_c = row_c[['thresh', 'SEN', 'SPE', 'PPV', 'NPV', 'F1']].round(3)\n",
        "print(final_c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omBBcMFZd0To",
        "outputId": "35f1bb4a-13e9-482a-eb23-1fa3dda076f8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thresh    0.367\n",
            "SEN       0.622\n",
            "SPE       0.793\n",
            "PPV       0.642\n",
            "NPV       0.779\n",
            "F1        0.632\n",
            "Name: 568, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('test_boot.csv')\n",
        "\n",
        "# The first 500 columns are model probabilities\n",
        "prob_cols = df.columns[:-1]\n",
        "label_col = df.columns[-1]\n",
        "\n",
        "# Calculate mean prediction for each sample across 500 models\n",
        "p_mean = df[prob_cols].mean(axis=1).values\n",
        "# True label\n",
        "y_true = df[label_col].values\n",
        "# Calculate expected bias error for each sample\n",
        "bias_error = (p_mean - y_true) ** 2\n",
        "\n",
        "# Calculate expected variance error for each sample\n",
        "variance_error = df[prob_cols].var(axis=1).values\n",
        "\n",
        "# Mean bias and variance error\n",
        "expected_bias_error = np.round(np.mean(bias_error), 2)\n",
        "expected_variance_error = np.round(np.mean(variance_error), 2)\n",
        "\n",
        "print(f\"Expected bias error: {expected_bias_error}\")\n",
        "print(f\"Expected variance error: {expected_variance_error}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsSf8etSeXJB",
        "outputId": "76ecafa5-c221-4aaa-bcde-2d82dfd29f2d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected bias error: 0.22\n",
            "Expected variance error: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2D"
      ],
      "metadata": {
        "id": "BtOvnoFoe-pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('test_boot.csv')\n",
        "prob_cols = df.columns[:-1]\n",
        "label_col = df.columns[-1]\n",
        "y_true = df[label_col].values\n",
        "\n",
        "n_bins = 10\n",
        "bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "calib_results = []\n",
        "\n",
        "# For each model, compute calibration per bin\n",
        "for col in prob_cols:\n",
        "    prob = df[col].values\n",
        "    binids = np.digitize(prob, bin_edges) - 1\n",
        "    bin_means = []\n",
        "    frac_positives = []\n",
        "    for b in range(n_bins):\n",
        "        idx = binids == b\n",
        "        if np.any(idx):\n",
        "            bin_means.append(np.mean(prob[idx]))\n",
        "            frac_positives.append(np.mean(y_true[idx]))\n",
        "        else:\n",
        "            bin_means.append(np.nan)\n",
        "            frac_positives.append(np.nan)\n",
        "    calib_results.append((bin_means, frac_positives))\n",
        "\n",
        "# Convert to arrays\n",
        "bin_means_arr = np.array([x[0] for x in calib_results])\n",
        "frac_pos_arr = np.array([x[1] for x in calib_results])\n",
        "\n",
        "# Take statistics across models\n",
        "mean_pred_median = np.nanmedian(bin_means_arr, axis=0)\n",
        "frac_pos_median = np.nanmedian(frac_pos_arr, axis=0)\n",
        "frac_pos_low = np.nanpercentile(frac_pos_arr, 2.5, axis=0)\n",
        "frac_pos_high = np.nanpercentile(frac_pos_arr, 97.5, axis=0)\n",
        "\n",
        "# Plot and save figure\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(mean_pred_median, frac_pos_median, 'o-', label='Model median prediction')\n",
        "plt.fill_between(mean_pred_median, frac_pos_low, frac_pos_high, alpha=0.3, label='95% CI')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
        "plt.xlabel('Expected probability')\n",
        "plt.ylabel('Predicted probability')\n",
        "plt.title('Calibration Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Save as 600 dpi PNG\n",
        "plt.savefig(\"calibration_curve.png\", dpi=600, bbox_inches='tight')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_f2TeIT0DgD",
        "outputId": "e9e93da5-60bb-400e-9603-ab22f71fd61d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-27-2797629699.py:36: RuntimeWarning: All-NaN slice encountered\n",
            "  mean_pred_median = np.nanmedian(bin_means_arr, axis=0)\n",
            "/tmp/ipython-input-27-2797629699.py:37: RuntimeWarning: All-NaN slice encountered\n",
            "  frac_pos_median = np.nanmedian(frac_pos_arr, axis=0)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1634: RuntimeWarning: All-NaN slice encountered\n",
            "  return fnb._ureduce(a,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('test_boot.csv')\n",
        "prob_cols = df.columns[:-1]\n",
        "label_col = df.columns[-1]\n",
        "y_true = df[label_col].values\n",
        "\n",
        "n_bins = 10\n",
        "bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "min_samples = 10  # Minimum number of samples in a bin to display\n",
        "\n",
        "calib_results = []\n",
        "bin_sample_counts = []\n",
        "\n",
        "# For each model, compute calibration per bin with sample count\n",
        "for col in prob_cols:\n",
        "    prob = df[col].values\n",
        "    binids = np.digitize(prob, bin_edges) - 1\n",
        "    bin_means = []\n",
        "    frac_positives = []\n",
        "    sample_counts = []\n",
        "    for b in range(n_bins):\n",
        "        idx = binids == b\n",
        "        sample_count = np.sum(idx)\n",
        "        sample_counts.append(sample_count)\n",
        "        if sample_count >= min_samples:\n",
        "            bin_means.append(np.mean(prob[idx]))\n",
        "            frac_positives.append(np.mean(y_true[idx]))\n",
        "        else:\n",
        "            bin_means.append(np.nan)\n",
        "            frac_positives.append(np.nan)\n",
        "    calib_results.append((bin_means, frac_positives))\n",
        "    bin_sample_counts.append(sample_counts)\n",
        "\n",
        "# Convert to arrays\n",
        "bin_means_arr = np.array([x[0] for x in calib_results])\n",
        "frac_pos_arr = np.array([x[1] for x in calib_results])\n",
        "bin_sample_counts_arr = np.array(bin_sample_counts)\n",
        "\n",
        "# Only keep bins where at least half the models have enough samples\n",
        "enough_samples = (np.sum(~np.isnan(frac_pos_arr), axis=0) > (len(prob_cols) // 2))\n",
        "\n",
        "# Calculate statistics for valid bins only\n",
        "mean_pred_median = np.nanmedian(bin_means_arr[:, enough_samples], axis=0)\n",
        "frac_pos_median = np.nanmedian(frac_pos_arr[:, enough_samples], axis=0)\n",
        "frac_pos_low = np.nanpercentile(frac_pos_arr[:, enough_samples], 2.5, axis=0)\n",
        "frac_pos_high = np.nanpercentile(frac_pos_arr[:, enough_samples], 97.5, axis=0)\n",
        "\n",
        "# Bin centers for x-axis\n",
        "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "bin_centers_valid = bin_centers[enough_samples]\n",
        "\n",
        "# Plot calibration curve with CI, only for valid bins\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(bin_centers_valid, frac_pos_median, 'o-', label='Model median prediction')\n",
        "plt.fill_between(bin_centers_valid, frac_pos_low, frac_pos_high, alpha=0.3, label='95% CI')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
        "plt.xlabel('Expected probability')\n",
        "plt.ylabel('Fraction of positives')\n",
        "plt.title('Calibration Curve (filtered, min 10 samples per bin)')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"calibration_curve_filtered.png\", dpi=600, bbox_inches='tight')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "jAugY0Hc2A1r"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}