{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbRxg5WMLcwT",
        "outputId": "f2477160-393c-484b-ea89-7ba297da0fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output y = 0.9740769841801067\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define activation functions\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)  # ReLU = max(0, z)\n",
        "\n",
        "def softplus(z):\n",
        "    return np.log(1 + np.exp(z))  # Softplus = log(1 + exp(z))\n",
        "\n",
        "# Forward propagation function\n",
        "def forward_pass(x, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(W1, x) + b1       # Linear combination at hidden layer\n",
        "    A1 = relu(Z1)                 # ReLU activation\n",
        "    Z2 = np.dot(W2, A1) + b2      # Linear combination at output layer\n",
        "    A2 = softplus(Z2)             # Softplus activation\n",
        "    return A2                     # Final output\n",
        "\n",
        "x = np.array([0, 0])\n",
        "\n",
        "# Weights and biases from input layer to hidden layer\n",
        "W1 = np.array([[2, -1],          # Weights to h1\n",
        "               [-1, 2],          # Weights to h2\n",
        "               [1, 1]])          # Weights to h3\n",
        "b1 = np.array([0.5, -0.5, 0.0])  # Biases for h1, h2, h3\n",
        "\n",
        "# Weights and bias from hidden layer to output layer\n",
        "W2 = np.array([[1, -1, 0.5]])    # Weights from h1, h2, h3 to y\n",
        "b2 = np.array([0.0])             # Bias for output node y\n",
        "\n",
        "# Run the forward pass\n",
        "output = forward_pass(x, W1, b1, W2, b2)\n",
        "print(\"Model output y =\", output[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define activation functions\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)  # ReLU = max(0, z)\n",
        "\n",
        "def softplus(z):\n",
        "    return np.log(1 + np.exp(z))  # Softplus = log(1 + exp(z))\n",
        "\n",
        "# Forward propagation function\n",
        "def forward_pass(x, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(W1, x) + b1       # Linear combination at hidden layer\n",
        "    A1 = relu(Z1)                 # ReLU activation\n",
        "    Z2 = np.dot(W2, A1) + b2      # Linear combination at output layer\n",
        "    A2 = softplus(Z2)             # Softplus activation\n",
        "    return A2                     # Final output\n",
        "\n",
        "x = np.array([-0.2, 0.7])\n",
        "\n",
        "# Weights and biases from input layer to hidden layer\n",
        "W1 = np.array([[2, -1],          # Weights to h1\n",
        "               [-1, 2],          # Weights to h2\n",
        "               [1, 1]])          # Weights to h3\n",
        "b1 = np.array([0.5, -0.5, 0.0])  # Biases for h1, h2, h3\n",
        "\n",
        "# Weights and bias from hidden layer to output layer\n",
        "W2 = np.array([[1, -1, 0.5]])    # Weights from h1, h2, h3 to y\n",
        "b2 = np.array([0.0])             # Bias for output node y\n",
        "\n",
        "# Run the forward pass\n",
        "output = forward_pass(x, W1, b1, W2, b2)\n",
        "print(\"Model output y =\", output[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-3IgR7wNxsg",
        "outputId": "48c93b41-09e4-4eb4-be04-74a84052f4b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output y = 0.35586506844219595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define activation functions\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)  # ReLU = max(0, z)\n",
        "\n",
        "def softplus(z):\n",
        "    return np.log(1 + np.exp(z))  # Softplus = log(1 + exp(z))\n",
        "\n",
        "# Forward propagation function\n",
        "def forward_pass(x, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(W1, x) + b1       # Linear combination at hidden layer\n",
        "    A1 = relu(Z1)                 # ReLU activation\n",
        "    Z2 = np.dot(W2, A1) + b2      # Linear combination at output layer\n",
        "    A2 = softplus(Z2)             # Softplus activation\n",
        "    return A2                     # Final output\n",
        "\n",
        "x = np.array([0.5, 0.5])\n",
        "\n",
        "# Weights and biases from input layer to hidden layer\n",
        "W1 = np.array([[2, -1],          # Weights to h1\n",
        "               [-1, 2],          # Weights to h2\n",
        "               [1, 1]])          # Weights to h3\n",
        "b1 = np.array([0.5, -0.5, 0.0])  # Biases for h1, h2, h3\n",
        "\n",
        "# Weights and bias from hidden layer to output layer\n",
        "W2 = np.array([[1, -1, 0.5]])    # Weights from h1, h2, h3 to y\n",
        "b2 = np.array([0.0])             # Bias for output node y\n",
        "\n",
        "# Run the forward pass\n",
        "output = forward_pass(x, W1, b1, W2, b2)\n",
        "print(\"Model output y =\", output[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPIGzWu4N4fZ",
        "outputId": "f11d9a32-bbaa-4c99-ccff-443cb535db26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output y = 1.7014132779827524\n"
          ]
        }
      ]
    }
  ]
}